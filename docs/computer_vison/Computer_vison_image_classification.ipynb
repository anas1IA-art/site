{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification\n"
      ],
      "metadata": {
        "id": "Z5pbBYbcG5hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation\n",
        "\n",
        "In this section, we will introduce the Image Classification problem, which involves assigning an input image a label from a fixed set of categories. Despite its apparent simplicity, this core problem in Computer Vision has numerous practical applications. Additionally, many other Computer Vision tasks, such as object detection and segmentation, can be reduced to image classification.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRWL_7eOKLFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "\n",
        "\n",
        "In the image below,\n",
        "an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}.\n",
        "\n",
        " $ \\text{Single Image} \\rightarrow \\text{Model Processes} \\rightarrow \\{ \\text{Cat}, \\text{Dog}, \\text{Hat}, \\text{Mug} \\} $.\n",
        "\n",
        "As illustrated, an image is represented to a computer as a large 3-dimensional tensor( PyTorch ([tutorial](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html))) of numbers. For example, consider an image of a cat that is 248 pixels wide, 400 pixels tall, and has three color channelsâ€”Red, Green, and Blue (RGB). Consequently, the image consists of \\( 248 \\times 400 \\times 3 \\) numbers, totaling 297,600 numbers. Each number represents an integer value ranging from 0 (black) to 255 (white).\n",
        "\n",
        "\n",
        "**Our task is to convert this array of quarter of a million numbers into a single label, such as \"cat.\"**\n",
        "\n"
      ],
      "metadata": {
        "id": "bgO4_BOeNO0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![Image Description](https://cs231n.github.io/assets/classify.png)\n",
        "\n",
        "The task in Image Classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image. Images are 3-dimensional arrays of integers from 0 to 255, of size Width x Height x 3. The 3 represents the three color channels Red, Green, Blue."
      ],
      "metadata": {
        "id": "x6EYFKf2MI31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenges\n",
        "\n",
        "Since the task of recognizing a visual concept (e.g., cat) is relatively trivial for a human, it's important to consider the challenges involved from the perspective of a computer vision algorithm. Below, we present (an inexhaustive) list of challenges, keeping in mind that the raw representation of images is a 3-D array of brightness values:\n",
        "\n",
        "1. **Variability in Appearance**: Images of the same object can vary significantly in terms of lighting, angle, and background.\n",
        "2. **Scale and Resolution**: Objects can appear at different sizes and resolutions within an image.\n",
        "3. **Occlusion**: Parts of the object may be hidden or obscured by other objects.\n",
        "4. **Deformation**: Objects may be distorted or transformed in various ways.\n",
        "5. **Background Complexity**: Complex or cluttered backgrounds can make it difficult to isolate the object of interest.\n",
        "6. **Color and Lighting Conditions**: Variations in color and lighting can affect the appearance of the object.\n",
        "7. **Image Noise**: Noise and distortions in the image can interfere with object recognition.\n",
        "\n",
        "![Image Description](https://cs231n.github.io/assets/challenges.jpeg)\n",
        "\n"
      ],
      "metadata": {
        "id": "kyKk7IiOTM1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is there a robust image classification model capable of distinguishing between different classes effectively?**"
      ],
      "metadata": {
        "id": "VHtQN2HUT029"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer** : if there are model can do that ( distinguishing between different classes effectively) ,this model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations."
      ],
      "metadata": {
        "id": "L6enqwp5VUu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data- Driven approach"
      ],
      "metadata": {
        "id": "sfUzzhvPXoSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Challenge**: Writing an algorithm to classify images into categories is not straightforward, unlike simpler tasks like sorting numbers.\n",
        "- **Data-Driven Approach**: Instead of coding explicit rules for each category, we use a data-driven approach.\n",
        "- **Process**:\n",
        "  - Provide the computer with numerous labeled examples of each category.\n",
        "  - Develop learning algorithms that analyze these examples to understand visual characteristics.\n",
        "- **Training Dataset**: Accumulate a dataset of labeled images to train the algorithm.\n",
        "- **Goal**: Enable the algorithm to learn and classify images based on visual features from the examples provided.\n"
      ],
      "metadata": {
        "id": "YBmQQ-K4YYtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Here is an example of what such a dataset might look like*:\n",
        "\n",
        "\n",
        "![Descrption image](https://cs231n.github.io/assets/trainset.jpg)\n",
        "\n",
        "An example training set for four visual categories. In practice we may have thousands of categories and hundreds of thousands of images for each category."
      ],
      "metadata": {
        "id": "SOMsAYXeYk2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Image Classification Pipeline**\n",
        "\n",
        "1. **Input**: We start with a dataset consisting of \\( N \\) images, each labeled with one of \\( K \\) different classes. This dataset is known as the training set.\n",
        "\n",
        "2. **Learning**: The goal is to use the training set to learn the characteristics of each class. This process is known as training a classifier or learning a model.\n",
        "\n",
        "3. **Evaluation**: Finally, we assess the performance of the classifier by having it predict labels for a new set of images that it has not encountered before. We compare these predicted labels with the true labels of the images, referred to as the ground truth. The objective is to have the classifier's predictions closely match the true labels.\n"
      ],
      "metadata": {
        "id": "lumqwzE3ZjHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nearest Neighbor Classifier\n",
        "\n",
        "To start, we'll build a Nearest Neighbor Classifier. Although it's rarely used in real-world scenarios, it will help us grasp the basic idea of image classification."
      ],
      "metadata": {
        "id": "Uk8oW5FOaeod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Dataset**: CIFAR-10\n",
        "  - **Description**: The CIFAR-10 dataset is a popular toy image classification dataset.\n",
        "  - **Content**: It consists of 60,000 tiny images, each 32x32 pixels in size.\n",
        "  - **Labels**: Each image is labeled with one of 10 classes (e.g., airplane, automobile, bird, etc.).\n",
        "  - **Partition**:\n",
        "    - **Training Set**: 50,000 images\n",
        "    - **Test Set**: 10,000 images\n",
        "\n",
        "**Example**: The image below shows 10 random example images from each of the 10 classes.\n",
        "\n",
        "![Descrption image](https://cs231n.github.io/assets/nn.jpg)\n",
        "\n",
        "Left: Example images from the CIFAR-10 dataset. Right: first column shows a few test images and next to each we show the top 10 nearest neighbors in the training set according to pixel-wise difference."
      ],
      "metadata": {
        "id": "Fxv1Bkqgeniu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Spliting data**: Given the CIFAR-10 training set of 50,000 images (5,000 images per label), we aim to label the remaining 10,000 test images.\n",
        "- **Nearest Neighbor Classifier**:\n",
        "  - **Process**: The classifier compares a test image to every training image and predicts the label of the closest training image.\n",
        "  - **Example Result**: The image above and on the right shows results for 10 example test images.\n",
        "  - **Observation**:\n",
        "    - Only about 3 out of 10 test images are correctly classified as the same class.\n",
        "    - In 7 out of 10 cases, the predicted label is incorrect.\n",
        "    - **Example**: In the 8th row, the nearest training image to a horse head is a red car, likely due to a similar background, resulting in the horse being mislabeled as a car.\n",
        "\n",
        "You may have noticed that we left unspecified the details of exactly how we compare two images\n",
        "   \n",
        "   - **Approach**: One basic method is to compare the images pixel by pixel and sum up all the differences.\n",
        "  - **Representation**: Images are represented as vectors \\( I_1 \\) and \\( I_2 \\).\n",
        "  - **Comparison Metric**: A reasonable choice for comparing the images is the L1 distance.\n",
        "\n"
      ],
      "metadata": {
        "id": "XSVz7fnagS98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The L1 distance between two images \\( I_1 \\) and \\( I_2 \\) can be defined as:\n",
        "\n",
        "$ d_1(I_1, I_2) = \\sum_p |I_{p1} - I_{p2}| $\n",
        "\n",
        "where:\n",
        "- $I_{p1} $ and $ I_{p2} $ are the pixel values of the images \\( I_1 \\) and \\( I_2 \\) at position \\( p \\), respectively.\n",
        "- The sum is taken over all pixel positions \\( p \\).\n",
        "\n",
        "![Description Image](https://cs231n.github.io/assets/nneg.jpeg)\n",
        "\n",
        "An example of using pixel-wise differences to compare two images with L1 distance (for one color channel in this example). Two images are subtracted elementwise and then all differences are added up to a single number. If two images are identical the result will be zero. But if the images are very different the result will be large."
      ],
      "metadata": {
        "id": "vNc0A1XAiPOl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9uUuv3QCgVvN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}