{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"computer_vison/Computer_vison_image_classification/","title":"Image classification","text":"<p>The task in Image Classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image. Images are 3-dimensional arrays of integers from 0 to 255, of size Width x Height x 3. The 3 represents the three color channels Red, Green, Blue.</p> <p>Is there a robust image classification model capable of distinguishing between different classes effectively?</p> <p>answer : if there are model can do that ( distinguishing between different classes effectively) ,this model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.</p> <ul> <li>Challenge: Writing an algorithm to classify images into categories is not straightforward, unlike simpler tasks like sorting numbers.</li> <li>Data-Driven Approach: Instead of coding explicit rules for each category, we use a data-driven approach.</li> <li>Process:<ul> <li>Provide the computer with numerous labeled examples of each category.</li> <li>Develop learning algorithms that analyze these examples to understand visual characteristics.</li> </ul> </li> <li>Training Dataset: Accumulate a dataset of labeled images to train the algorithm.</li> <li>Goal: Enable the algorithm to learn and classify images based on visual features from the examples provided.</li> </ul> <p>Here is an example of what such a dataset might look like:</p> <p></p> <p>An example training set for four visual categories. In practice we may have thousands of categories and hundreds of thousands of images for each category.</p> <p>The Image Classification Pipeline</p> <ol> <li><p>Input: We start with a dataset consisting of ( N ) images, each labeled with one of ( K ) different classes. This dataset is known as the training set.</p> </li> <li><p>Learning: The goal is to use the training set to learn the characteristics of each class. This process is known as training a classifier or learning a model.</p> </li> <li><p>Evaluation: Finally, we assess the performance of the classifier by having it predict labels for a new set of images that it has not encountered before. We compare these predicted labels with the true labels of the images, referred to as the ground truth. The objective is to have the classifier's predictions closely match the true labels.</p> </li> </ol> <ul> <li>Dataset: CIFAR-10<ul> <li>Description: The CIFAR-10 dataset is a popular toy image classification dataset.</li> <li>Content: It consists of 60,000 tiny images, each 32x32 pixels in size.</li> <li>Labels: Each image is labeled with one of 10 classes (e.g., airplane, automobile, bird, etc.).</li> <li>Partition:<ul> <li>Training Set: 50,000 images</li> <li>Test Set: 10,000 images</li> </ul> </li> </ul> </li> </ul> <p>Example: The image below shows 10 random example images from each of the 10 classes.</p> <p></p> <p>Left: Example images from the CIFAR-10 dataset. Right: first column shows a few test images and next to each we show the top 10 nearest neighbors in the training set according to pixel-wise difference.</p> <ul> <li>Spliting data: Given the CIFAR-10 training set of 50,000 images (5,000 images per label), we aim to label the remaining 10,000 test images.</li> <li>Nearest Neighbor Classifier:<ul> <li>Process: The classifier compares a test image to every training image and predicts the label of the closest training image.</li> <li>Example Result: The image above and on the right shows results for 10 example test images.</li> <li>Observation:<ul> <li>Only about 3 out of 10 test images are correctly classified as the same class.</li> <li>In 7 out of 10 cases, the predicted label is incorrect.</li> <li>Example: In the 8th row, the nearest training image to a horse head is a red car, likely due to a similar background, resulting in the horse being mislabeled as a car.</li> </ul> </li> </ul> </li> </ul> <p>You may have noticed that we left unspecified the details of exactly how we compare two images</p> <ul> <li>Approach: One basic method is to compare the images pixel by pixel and sum up all the differences.</li> <li>Representation: Images are represented as vectors ( I_1 ) and ( I_2 ).</li> <li>Comparison Metric: A reasonable choice for comparing the images is the L1 distance.</li> </ul> <p>The L1 distance between two images ( I_1 ) and ( I_2 ) can be defined as:</p> <p>$ d_1(I_1, I_2) = \\sum_p |I_{p1} - I_{p2}| $</p> <p>where:</p> <ul> <li>$I_{p1} $ and $ I_{p2} $ are the pixel values of the images ( I_1 ) and ( I_2 ) at position ( p ), respectively.</li> <li>The sum is taken over all pixel positions ( p ).</li> </ul> <p></p> <p>An example of using pixel-wise differences to compare two images with L1 distance (for one color channel in this example). Two images are subtracted elementwise and then all differences are added up to a single number. If two images are identical the result will be zero. But if the images are very different the result will be large.</p> In\u00a0[\u00a0]: Copied!"},{"location":"computer_vison/Computer_vison_image_classification/#image-classification","title":"Image classification\u00b6","text":""},{"location":"computer_vison/Computer_vison_image_classification/#motivation","title":"Motivation\u00b6","text":"<p>In this section, we will introduce the Image Classification problem, which involves assigning an input image a label from a fixed set of categories. Despite its apparent simplicity, this core problem in Computer Vision has numerous practical applications. Additionally, many other Computer Vision tasks, such as object detection and segmentation, can be reduced to image classification.</p>"},{"location":"computer_vison/Computer_vison_image_classification/#example","title":"Example\u00b6","text":"<p>In the image below, an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}.</p> <p>$ \\text{Single Image} \\rightarrow \\text{Model Processes} \\rightarrow \\{ \\text{Cat}, \\text{Dog}, \\text{Hat}, \\text{Mug} \\} $.</p> <p>As illustrated, an image is represented to a computer as a large 3-dimensional tensor( PyTorch (tutorial)) of numbers. For example, consider an image of a cat that is 248 pixels wide, 400 pixels tall, and has three color channels\u2014Red, Green, and Blue (RGB). Consequently, the image consists of ( 248 \\times 400 \\times 3 ) numbers, totaling 297,600 numbers. Each number represents an integer value ranging from 0 (black) to 255 (white).</p> <p>Our task is to convert this array of quarter of a million numbers into a single label, such as \"cat.\"</p>"},{"location":"computer_vison/Computer_vison_image_classification/#challenges","title":"Challenges\u00b6","text":"<p>Since the task of recognizing a visual concept (e.g., cat) is relatively trivial for a human, it's important to consider the challenges involved from the perspective of a computer vision algorithm. Below, we present (an inexhaustive) list of challenges, keeping in mind that the raw representation of images is a 3-D array of brightness values:</p> <ol> <li>Variability in Appearance: Images of the same object can vary significantly in terms of lighting, angle, and background.</li> <li>Scale and Resolution: Objects can appear at different sizes and resolutions within an image.</li> <li>Occlusion: Parts of the object may be hidden or obscured by other objects.</li> <li>Deformation: Objects may be distorted or transformed in various ways.</li> <li>Background Complexity: Complex or cluttered backgrounds can make it difficult to isolate the object of interest.</li> <li>Color and Lighting Conditions: Variations in color and lighting can affect the appearance of the object.</li> <li>Image Noise: Noise and distortions in the image can interfere with object recognition.</li> </ol> <p></p>"},{"location":"computer_vison/Computer_vison_image_classification/#data-driven-approach","title":"Data- Driven approach\u00b6","text":""},{"location":"computer_vison/Computer_vison_image_classification/#nearest-neighbor-classifier","title":"Nearest Neighbor Classifier\u00b6","text":"<p>To start, we'll build a Nearest Neighbor Classifier. Although it's rarely used in real-world scenarios, it will help us grasp the basic idea of image classification.</p>"},{"location":"llms/","title":"Large Language Models","text":""},{"location":"llms/Build_chat_gpt_from_scratch/","title":"Build chat gpt from scratch","text":"<p>GPT (Generative Pre-trained Transformer) is a type of large language model developed by OpenAI that uses transformer architecture to generate human-like text based on input prompts. Its development was based on the paper \"Improving Language Understanding by Generative Pre-Training\" published by OpenAI in 2018.</p> <p>The groundbreaking transformer model, which GPT builds upon, was introduced in the paper \"Attention is All You Need\" by Vaswani et al., published in 2017.</p> <p>This notebook contains the notes of Andrej Karpathy under the title of Let's Build GPT: From Scratch, in Code, Spelled Out on his YouTube channel.</p> <p>In this tutorial, we utilize the Tiny Shakespeare dataset, a 1.06 MB text file that combines all the works of William Shakespeare.</p> In\u00a0[\u00a0]: Copied! <pre>!wget  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n</pre> !wget  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt <pre>--2024-06-09 15:28:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: \u2018input.txt\u2019\n\ninput.txt           100%[===================&gt;]   1.06M  --.-KB/s    in 0.06s   \n\n2024-06-09 15:28:47 (18.6 MB/s) - \u2018input.txt\u2019 saved [1115394/1115394]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Specifying `encoding='utf-8'` ensures that text files are read or written using the UTF-8 encoding, which supports a wide range of characters from various languages. Without this specification, the default system encoding is used, which can lead to inconsistencies and errors, especially with non-ASCII characters.\nwith open('./input.txt','r',encoding  = 'utf-8') as f :\n  text = f.read()\n</pre> # Specifying `encoding='utf-8'` ensures that text files are read or written using the UTF-8 encoding, which supports a wide range of characters from various languages. Without this specification, the default system encoding is used, which can lead to inconsistencies and errors, especially with non-ASCII characters. with open('./input.txt','r',encoding  = 'utf-8') as f :   text = f.read() In\u00a0[\u00a0]: Copied! <pre>print(len(text))\n</pre> print(len(text)) <pre>1115394\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(text[:11])\n</pre> print(text[:11]) <pre>First Citiz\n</pre> In\u00a0[\u00a0]: Copied! <pre># here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvoc_size = len(chars)\nprint(''.join(chars))\n</pre> # here are all the unique characters that occur in this text chars = sorted(list(set(text))) voc_size = len(chars) print(''.join(chars))  <pre>\n !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n</pre> In\u00a0[\u00a0]: Copied! <pre># create a mapping from characters to integers using The `enumerate` function in Python adds a counter to an iterable and returns it as an enumerate object, providing both index and value pairs in loops.\nint_char = {i:ch for i,ch in enumerate(chars , start = 0)}\nchar_int = {ch:i for i,ch in enumerate(chars , start = 0)}\nint_char[9],char_int['3']\n</pre> # create a mapping from characters to integers using The `enumerate` function in Python adds a counter to an iterable and returns it as an enumerate object, providing both index and value pairs in loops. int_char = {i:ch for i,ch in enumerate(chars , start = 0)} char_int = {ch:i for i,ch in enumerate(chars , start = 0)} int_char[9],char_int['3'] Out[\u00a0]: <pre>('3', 9)</pre> In\u00a0[\u00a0]: Copied! <pre>sentence  = 'Anas Nouri'\n# encoder = {} # trasform a sentenece into presentation numeric\nencoder = lambda s : [char_int[a]for a in s]\n# encoder(sentence)\ndecoder  =  lambda s : ''.join([int_char[a] for a in s ])\ndecoder(encoder(sentence))\n</pre> sentence  = 'Anas Nouri' # encoder = {} # trasform a sentenece into presentation numeric encoder = lambda s : [char_int[a]for a in s] # encoder(sentence) decoder  =  lambda s : ''.join([int_char[a] for a in s ]) decoder(encoder(sentence)) Out[\u00a0]: <pre>'Anas Nouri'</pre> In\u00a0[\u00a0]: Copied! <pre>encoder(text[:10])\n</pre> encoder(text[:10]) Out[\u00a0]: <pre>[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]</pre> In\u00a0[\u00a0]: Copied! <pre># let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch\n</pre> # let's now encode the entire text dataset and store it into a torch.Tensor import torch In\u00a0[\u00a0]: Copied! <pre>dataset = torch.tensor(encoder(text),dtype = torch.long)\ndataset\n</pre> dataset = torch.tensor(encoder(text),dtype = torch.long) dataset Out[\u00a0]: <pre>tensor([18, 47, 56,  ..., 45,  8,  0])</pre> In\u00a0[\u00a0]: Copied! <pre>dataset.unsqueeze(0)\n</pre> dataset.unsqueeze(0) Out[\u00a0]: <pre>tensor([[18, 47, 56,  ..., 45,  8,  0]])</pre> In\u00a0[\u00a0]: Copied! <pre>dataset.squeeze(0)\n</pre> dataset.squeeze(0) Out[\u00a0]: <pre>tensor([18, 47, 56,  ..., 45,  8,  0])</pre> In\u00a0[\u00a0]: Copied! <pre># Let's now split up the data into train and validation sets\nn = int(0.8*len(dataset))\ntrain_data = dataset[:n]\nvalidation_data = dataset[n:]\n</pre> # Let's now split up the data into train and validation sets n = int(0.8*len(dataset)) train_data = dataset[:n] validation_data = dataset[n:] In\u00a0[\u00a0]: Copied! <pre>Block_size = 10\ntrain_data = dataset[:Block_size]\ntarget_data = dataset[1:Block_size+1]\ntrain_data\n</pre> Block_size = 10 train_data = dataset[:Block_size] target_data = dataset[1:Block_size+1] train_data Out[\u00a0]: <pre>tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])</pre> In\u00a0[\u00a0]: Copied! <pre>target_data\n</pre> target_data Out[\u00a0]: <pre>tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64])</pre> In\u00a0[\u00a0]: Copied! <pre>len(target_data),len(train_data)\n</pre> len(target_data),len(train_data) Out[\u00a0]: <pre>(10, 10)</pre> In\u00a0[\u00a0]: Copied! <pre>for t in range(Block_size):\n    x = train_data[:t+1]\n    y = target_data[t]\n    print(f\"input is {x} and target is {y}\")\n</pre> for t in range(Block_size):     x = train_data[:t+1]     y = target_data[t]     print(f\"input is {x} and target is {y}\") <pre>input is tensor([18]) and target is 47\ninput is tensor([18, 47]) and target is 56\ninput is tensor([18, 47, 56]) and target is 57\ninput is tensor([18, 47, 56, 57]) and target is 58\ninput is tensor([18, 47, 56, 57, 58]) and target is 1\ninput is tensor([18, 47, 56, 57, 58,  1]) and target is 15\ninput is tensor([18, 47, 56, 57, 58,  1, 15]) and target is 47\ninput is tensor([18, 47, 56, 57, 58,  1, 15, 47]) and target is 58\ninput is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) and target is 47\ninput is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]) and target is 64\n</pre> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(1337)\nbatch_size = 6 # how many independent sequences will we process in parallel?\nblock_size = 10 # what is the maximum context length for predictions?\n</pre> torch.manual_seed(1337) batch_size = 6 # how many independent sequences will we process in parallel? block_size = 10 # what is the maximum context length for predictions? In\u00a0[\u00a0]: Copied! <pre>ix = torch.randint(len(dataset) - block_size, (batch_size,))\n</pre> ix = torch.randint(len(dataset) - block_size, (batch_size,)) In\u00a0[\u00a0]: Copied! <pre>ix\n</pre> ix Out[\u00a0]: <pre>tensor([1080343,  458285,   42868,  672888, 1083415,  245809])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"llms/Build_chat_gpt_from_scratch/#building-chatgpt-on-based-on-paper-attention-is-all-you-need","title":"Building Chatgpt on based on paper \"Attention is all you need\"\u00b6","text":""},{"location":"llms/Build_chat_gpt_from_scratch/#lets-prepare-dataset","title":"Let's  Prepare dataset\u00b6","text":""}]}